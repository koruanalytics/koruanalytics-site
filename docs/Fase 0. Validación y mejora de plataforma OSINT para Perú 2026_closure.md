# AcciÃ³n de Cierre de Chat del Proyecto: OSINT PerÃº 2026 - AuditorÃ­a de Calidad

## Metadatos del Chat
| Campo | Valor |
|-------|-------|
| **Nombre del proyecto** | OSINT PerÃº 2026 - Electoral Security Monitoring Platform |
| **Fecha de inicio del chat** | 2025-01-05 |
| **Fecha de cierre** | 2025-01-05 |
| **DuraciÃ³n estimada** | 1 sesiÃ³n / ~30 minutos |
| **Chat ID/Referencia** | AuditorÃ­a inicial arquitectura y cÃ³digo |

---

## 1. RESUMEN EJECUTIVO
Inicio de auditorÃ­a completa de calidad e integridad para plataforma OSINT de monitoreo electoral PerÃº 2026. Sistema funcional en local (Windows/DuckDB/Python) procesando 650+ artÃ­culos diarios desde 7 fuentes peruanas, identificando ~210 incidentes reales con 71% de filtrado de ruido. Primera auditorÃ­a revelÃ³ 4 issues crÃ­ticos en orquestaciÃ³n del pipeline. Pendiente: recibir schemas completos DuckDB y cÃ³digo de 4 scripts core para auditorÃ­a code-level detallada.

---

## 2. STACK TECNOLÃ“GICO

| CategorÃ­a | TecnologÃ­a | VersiÃ³n |
|-----------|------------|---------|
| Lenguaje | Python | 3.x (.venv) |
| Base de datos | DuckDB | Latest (columnar) |
| Logging | loguru | - |
| Ingesta APIs | NewsAPI.ai / EventRegistry | - |
| LLM Classification | Claude Haiku | API |
| OrquestaciÃ³n local | PowerShell + Task Scheduler | Windows |
| IDE | VS Code | - |
| OS | Windows 10/11 | - |
| Target Cloud | Azure (OpenAI, AI Search, Maps, Storage) | Parcialmente configurado |
| Arquitectura datos | Medallion (Bronzeâ†’Silverâ†’Gold) | DuckDB |

---

## 3. ESTRUCTURA DEL PROYECTO

```
C:\Users\carlo\OneDrive - KoruAnalytics\Prj_OSINT\2026_Peru\
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_incidents_job.py                    # âœ… Orquestador pipeline (auditado)
â”‚   â”œâ”€â”€ run_incident_extract_baseline.py        # ğŸ” Extrae incidentes desde stg_news
â”‚   â”œâ”€â”€ run_geo_resolve_incidents.py            # ğŸ” Geocoding contra dim_places_pe
â”‚   â”œâ”€â”€ build_fct_incidents.py                  # ğŸ” Construye fact table
â”‚   â”œâ”€â”€ check_incidents_run.py                  # ğŸ” Validaciones post-pipeline
â”‚   â”œâ”€â”€ run_newsapi_ai_job.py                   # âœ… Job ingesta NewsAPI
â”‚   â”œâ”€â”€ scheduled_run_newsapi_ai_job.ps1        # âœ… Wrapper PowerShell con lock
â”‚   â”œâ”€â”€ load_gazetteer_pe.py                    # âœ… Carga dim geogrÃ¡fica
â”‚   â”œâ”€â”€ reset_dim_places_pe.py                  # âœ… Reset dimensiÃ³n geo
â”‚   â”œâ”€â”€ inspect_dim_places_pe.py                # âš ï¸ Auditado - schemas incompletos
â”‚   â”œâ”€â”€ inspect_stg_incidents_extracted.py      # Inspect staging incidents
â”‚   â”œâ”€â”€ validate_gazetteer_pe.py                # âœ… ValidaciÃ³n gazetteer
â”‚   â”œâ”€â”€ write_gazetteer_checksums.py            # âœ… Genera SHA256
â”‚   â”œâ”€â”€ validate_gazetteer_checksum.py          # âœ… Verifica integridad
â”‚   â””â”€â”€ [legacy scripts - no usar sin validar]
â”œâ”€â”€ config/
â”‚   â””â”€â”€ geo/
â”‚       â”œâ”€â”€ peru_gazetteer_full.csv             # âœ… Gazetteer versionado
â”‚       â””â”€â”€ peru_gazetteer_full.csv.sha256      # âœ… Checksum integridad
â”œâ”€â”€ data/
â”‚   â””â”€â”€ osint_dw.duckdb                         # âœ… Data warehouse principal
â”œâ”€â”€ .venv/                                       # âœ… Python virtual environment
â”œâ”€â”€ pyproject.toml / requirements.txt           # ğŸ” Pendiente revisar
â””â”€â”€ README.md                                    # DocumentaciÃ³n proyecto
```

---

## 4. ARCHIVOS CLAVE Y SU ESTADO

| Archivo | Estado | DescripciÃ³n | Ãšltima modificaciÃ³n |
|---------|--------|-------------|---------------------|
| `run_incidents_job.py` | âš ï¸ Funcional pero mejorable | Orquesta 4 pasos con run_id, sin pre-checks ni logging robusto | 2025-01-05 |
| `inspect_dim_places_pe.py` | âš ï¸ Requiere revisiÃ³n | Count en lugar de DESCRIBE, no muestra schema real | 2025-01-05 |
| `run_incident_extract_baseline.py` | ğŸ” Pendiente auditar | ExtracciÃ³n incidentes: stg_news â†’ stg_incidents_extracted (22 cols) | - |
| `run_geo_resolve_incidents.py` | ğŸ” Pendiente auditar | Geocoding + escritura map_incident_place | - |
| `build_fct_incidents.py` | ğŸ” Pendiente auditar | Schema-aware, fact builder, override_* desactivados | - |
| `check_incidents_run.py` | ğŸ” Pendiente auditar | Validaciones integridad post-ejecuciÃ³n | - |
| `scheduled_run_newsapi_ai_job.ps1` | âœ… Completo | Lock anti-solape, parsea run_id, logs maestros | 2025-01-05 |
| `peru_gazetteer_full.csv` | âœ… Completo | DimensiÃ³n geo: ADM1/2/3, lat/lon, versionado + SHA256 | 2025-01-05 |
| `osint_dw.duckdb` | âœ… Operativo | DW principal con Medallion: stg â†’ dim â†’ fct â†’ map | 2025-01-05 |

**Leyenda:** âœ… Completo | ğŸ” Pendiente auditar | âš ï¸ Requiere revisiÃ³n | âŒ Issues crÃ­ticos

---

## 5. FUNCIONALIDADES IMPLEMENTADAS

### Completadas
- [x] **Ingesta NewsAPI** - 650+ artÃ­culos/dÃ­a desde 7 fuentes peruanas especÃ­ficas (source-based)
- [x] **DeduplicaciÃ³n robusta** - Por canonical/original URI en DuckDB staging
- [x] **DimensiÃ³n geogrÃ¡fica completa** - Gazetteer PerÃº versionado con checksum SHA256
- [x] **Pipeline incidentes 4 pasos** - Extract â†’ GEO â†’ Fact â†’ Check, EXIT_CODE=0 validado
- [x] **Trazabilidad run_id** - PropagaciÃ³n completa por toda la pipeline
- [x] **ClasificaciÃ³n LLM** - Claude Haiku: 14 event types, casualty counts, bilingual summaries
- [x] **Arquitectura Medallion** - Bronze/Silver/Gold implementada en DuckDB
- [x] **Scheduling automÃ¡tico** - Task Scheduler integrado con PowerShell wrappers
- [x] **ValidaciÃ³n gazetteer** - Scripts de verificaciÃ³n integridad y checksums
- [x] **Data quality tracking** - 408 muertes, 2,046 heridos tracked, 71% ruido filtrado

### En Progreso
- [ ] **AuditorÃ­a code-level** - RevisiÃ³n detallada de 4 scripts core (50% pendiente)
- [ ] **Azure migration prep** - Infraestructura parcialmente configurada (OpenAI, AI Search, Maps)

### Pendientes
- [ ] **Tabla curation_incident_overrides** - Flujo curaciÃ³n manual + override_* columns (Bloque H/F8)
- [ ] **GEO avanzado** - Multi-match, ranking/score, mejor extracciÃ³n location_text (Bloque H/G1)
- [ ] **ContainerizaciÃ³n Docker** - PreparaciÃ³n para Azure Container Instances
- [ ] **Frontend** - Power BI Pro + ArcGIS + React + RAG chat
- [ ] **Monitoring & alerting** - Application Insights + alertas proactivas
- [ ] **Tests automatizados** - Suite de unit tests y integration tests
- [ ] **CI/CD pipeline** - Despliegue automatizado a Azure

---

## 6. CONFIGURACIÃ“N Y VARIABLES DE ENTORNO

```bash
# Variables requeridas (.env o sistema)
NEWSAPI_AI_KEY=your_newsapi_key               # API key para ingesta NewsAPI.ai
AZURE_OPENAI_KEY=your_azure_openai_key        # Para clasificaciÃ³n LLM
AZURE_AI_SEARCH_KEY=your_search_key           # Azure AI Search (RAG futuro)
AZURE_MAPS_KEY=your_maps_key                  # Azure Maps (geocoding avanzado)

# Paths locales
DUCKDB_PATH=data/osint_dw.duckdb              # DW principal
GAZETTEER_PATH=config/geo/peru_gazetteer_full.csv  # DimensiÃ³n geo

# ConfiguraciÃ³n pipeline
PYTHON_VENV=.venv/Scripts/python.exe          # Python virtual environment
LOG_LEVEL=INFO                                # Nivel logging (con loguru)
```

**Archivos de configuraciÃ³n:**
| Archivo | PropÃ³sito | Notas |
|---------|-----------|-------|
| `.env` | Variables de entorno sensibles | No commitear, gitignore |
| `peru_gazetteer_full.csv` | DimensiÃ³n geogrÃ¡fica completa | Versionado + SHA256 |
| `peru_gazetteer_full.csv.sha256` | Checksum integridad | Validar antes de cargar |
| `scheduled_run_newsapi_ai_job.ps1` | Task Scheduler wrapper | Lock anti-solape integrado |

---

## 7. DECISIONES TÃ‰CNICAS TOMADAS

| # | DecisiÃ³n | RazÃ³n | Alternativa descartada |
|---|----------|-------|------------------------|
| 1 | DuckDB en lugar de PostgreSQL | Simplicidad local + performance columnar + embeddable | PostgreSQL (overengineering para prototipo local) |
| 2 | Ingesta source-based (7 outlets peruanos) | AumentÃ³ volumen 6â†’650 artÃ­culos/dÃ­a con mejor calidad | Location-based queries (cobertura insuficiente) |
| 3 | Claude Haiku para clasificaciÃ³n | 71% filtrado ruido efectivo, costos controlados | Rule-based (10% precision) |
| 4 | Arquitectura Medallion (Bronzeâ†’Silverâ†’Gold) | Data quality + trazabilidad + analytics complejos | Flat tables (no escalable, no auditable) |
| 5 | Gazetteer como artefacto versionado local | Evita HTTP 403 en ejecuciÃ³n diaria, garantiza reproducibilidad | Download on-demand (API inestable) |
| 6 | PowerShell + Task Scheduler | Setup rÃ¡pido Windows, lock anti-solape robusto | Apache Airflow (overhead innecesario en local) |
| 7 | run_id como identificador universal | Trazabilidad end-to-end desde ingesta hasta fact | Timestamps (colisiones posibles) |
| 8 | SeparaciÃ³n Extractâ†’GEOâ†’Factâ†’Check | Debugging granular, reprocess selectivo posible | Pipeline monolÃ­tico (difÃ­cil debuggear) |

---

## 8. PROBLEMAS RESUELTOS

### [P1] Baja cobertura de ingesta (6 artÃ­culos/dÃ­a)
- **SÃ­ntoma:** Pipeline ingesta solo 6 artÃ­culos relevantes por dÃ­a, insuficiente para monitoreo
- **Causa raÃ­z:** Location-based queries muy especÃ­ficas generaban poca cobertura
- **SoluciÃ³n aplicada:**
  - Cambio estratÃ©gico: source-based ingestion targeting 7 Peruvian outlets
  - NewsAPI.ai configurado para scrape completo de fuentes especÃ­ficas
  - Resultado: 650+ artÃ­culos/dÃ­a, ~210 incidentes reales identificados
- **Archivos afectados:** `run_newsapi_ai_job.py`, configuraciÃ³n NewsAPI
- **LecciÃ³n aprendida:** Filtrar en source es mÃ¡s efectivo que filtrar en query para OSINT regional

### [P2] ClasificaciÃ³n rule-based con 90% ruido
- **SÃ­ntoma:** Sistema rule-based clasificaba 90% de artÃ­culos como incidentes (falsos positivos masivos)
- **Causa raÃ­z:** Keywords simples capturaban noticias generales de crimen/polÃ­tica sin valor OSINT
- **SoluciÃ³n aplicada:**
```python
# Reemplazo completo por LLM-based classification con Claude Haiku
# Extrae 14 event types especÃ­ficos + confidence + metadata estructurada
# Result: 71% noise filtering, solo 210 genuine incidents de 650+ articles
```
- **Archivos afectados:** `run_incident_extract_baseline.py` (extracciÃ³n LLM completa)
- **LecciÃ³n aprendida:** LLM classification >> rule-based para contenido no estructurado en espaÃ±ol

### [P3] HTTP 403 en descarga gazetteer durante ejecuciÃ³n diaria
- **SÃ­ntoma:** Pipeline fallaba aleatoriamente por rate limiting en API geogrÃ¡fica
- **Causa raÃ­z:** Gazetteer Peru descargado on-demand en cada ejecuciÃ³n
- **SoluciÃ³n aplicada:**
```python
# Gazetteer versionado como artefacto en repo
# config/geo/peru_gazetteer_full.csv + SHA256 checksum
# Scripts: validate_gazetteer_checksum.py antes de load_gazetteer_pe.py
# Update: manual solo cuando necesario, no en daily runs
```
- **Archivos afectados:** `load_gazetteer_pe.py`, `validate_gazetteer_checksum.py`
- **LecciÃ³n aprendida:** Artefactos crÃ­ticos deben ser versionados localmente, no dependientes de APIs externas

---

## 9. PROBLEMAS CONOCIDOS / DEUDA TÃ‰CNICA

| # | Problema | Impacto | SoluciÃ³n propuesta | Prioridad |
|---|----------|---------|-------------------|-----------|
| 1 | Scripts inspect no muestran schemas reales | Medio | Usar `DESCRIBE table_name` en lugar de `pragma_table_info` count | Media |
| 2 | Pipeline sin rollback en fallo parcial | Alto | Pre-flight checks + transaccionalidad + cleanup automÃ¡tico | Alta |
| 3 | Logging sin timestamps ni mÃ©tricas | Medio | Integrar loguru con duraciÃ³n steps + structured logging | Alta |
| 4 | No hay validaciÃ³n pre-vuelo | Alto | Checks: dim_places_pe populated, run_id no duplicado, DuckDB accessible | Alta |
| 5 | Schemas no documentados formalmente | Medio | Generar DDL completo + documentaciÃ³n ER diagram | Media |
| 6 | override_* columns no implementadas | Medio | Tabla `curation_incident_overrides` + flujo curaciÃ³n (Bloque H) | Media |
| 7 | Scripts legacy coexisten con actuales | Bajo | Mover a `/archive` o eliminar, documentar cuÃ¡les usar | Baja |
| 8 | No hay tests automatizados | Alto | Suite pytest: unit tests + integration tests para pipeline | Media |
| 9 | Alerting inexistente | Alto | Azure Application Insights + alertas por email/SMS en fallos | Alta |
| 10 | Logs no centralizados | Medio | AgregaciÃ³n logs en Azure Log Analytics o similar | Baja |

---

## 10. PRÃ“XIMOS PASOS (PRIORIZADO)

### Alta Prioridad
1. **[INMEDIATO]** Recibir schemas completos DuckDB para continuar auditorÃ­a
   - Ejecutar: `python -c "import duckdb; con=duckdb.connect('data/osint_dw.duckdb'); print(con.execute('DESCRIBE dim_places_pe').df()); print(con.execute('DESCRIBE stg_incidents_extracted').df()); print(con.execute('DESCRIBE fct_incidents').df())"`
   - Archivos: todos los `DESCRIBE` de tablas del Medallion

2. **[INMEDIATO]** Compartir cÃ³digo de 4 scripts core del pipeline
   - `run_incident_extract_baseline.py` (extracciÃ³n LLM)
   - `run_geo_resolve_incidents.py` (geocoding)
   - `build_fct_incidents.py` (fact builder)
   - `check_incidents_run.py` (validaciones)

3. **[CRÃTICO]** Implementar mejoras en `run_incidents_job.py`
   - Pre-flight checks function
   - Logging con loguru + timestamps + duraciÃ³n
   - Error handling robusto con stderr capture
   - Cleanup automÃ¡tico en caso de fallo parcial

### Media Prioridad
4. **[PRÃ“XIMA SESIÃ“N]** Completar auditorÃ­a code-level con cÃ³digo recibido
   - Revisar manejo errores, idempotencia, SQL injection risks
   - Validar data quality checks en cada paso
   - Verificar consistencia schemas vs cÃ³digo

5. **[PRÃ“XIMA SESIÃ“N]** Revisar dependencias y configuraciÃ³n
   - `pyproject.toml` o `requirements.txt` completo
   - Versiones pinned de librerÃ­as crÃ­ticas
   - Conflictos de dependencias potenciales

6. **[SIGUIENTE FASE]** PreparaciÃ³n Azure migration
   - Dockerfile para containerizaciÃ³n
   - Azure Key Vault para secretos
   - CI/CD pipeline bÃ¡sico (GitHub Actions / Azure DevOps)

### Baja Prioridad
7. **[CUANDO SEA POSIBLE]** Refactoring y optimizaciones
   - Type hints completos en todos los scripts
   - Docstrings exhaustivos (Google style)
   - Eliminar scripts legacy o mover a `/archive`

8. **[CUANDO SEA POSIBLE]** Testing automatizado
   - Unit tests con pytest
   - Integration tests para pipeline end-to-end
   - Mock de APIs externas (NewsAPI, Claude)

---

## 11. CÃ“DIGO CRÃTICO PARA REFERENCIA

### Mejora propuesta: run_incidents_job.py con pre-flight checks y logging robusto
```python
from __future__ import annotations

import argparse
import sys
from pathlib import Path
import subprocess
import time
import duckdb
from loguru import logger

PROJECT_ROOT = Path(__file__).resolve().parents[1]
PYTHON = str(PROJECT_ROOT / ".venv" / "Scripts" / "python.exe")

# Configure loguru
logger.add(
    PROJECT_ROOT / "logs" / "incidents_job_{time}.log",
    rotation="1 day",
    retention="30 days",
    level="INFO"
)

def preflight_checks(run_id: str) -> None:
    """
    Validate prerequisites before starting pipeline.
    
    Args:
        run_id: Unique identifier for this ingestion run
        
    Raises:
        SystemExit: If any prerequisite check fails
    """
    logger.info("Running pre-flight checks")
    
    con = duckdb.connect(str(PROJECT_ROOT / "data" / "osint_dw.duckdb"))
    
    try:
        # Check 1: dim_places_pe is populated
        places_count = con.execute(
            "SELECT COUNT(*) FROM dim_places_pe"
        ).fetchone()[0]
        
        if places_count == 0:
            logger.error("dim_places_pe is EMPTY - run load_gazetteer_pe.py first!")
            raise SystemExit(1)
        
        logger.info(f"âœ“ dim_places_pe populated: {places_count:,} places")
        
        # Check 2: Verify run_id not already processed
        existing_incidents = con.execute(
            "SELECT COUNT(*) FROM fct_incidents WHERE ingest_run_id = ?",
            [run_id]
        ).fetchone()[0]
        
        if existing_incidents > 0:
            logger.warning(
                f"run_id {run_id} already processed ({existing_incidents} incidents). "
                "Pipeline will reprocess (idempotent mode)."
            )
        else:
            logger.info(f"âœ“ run_id {run_id} is new")
        
        # Check 3: Verify staging data exists for this run_id
        staging_articles = con.execute(
            "SELECT COUNT(*) FROM stg_news_newsapi_ai WHERE run_id = ?",
            [run_id]
        ).fetchone()[0]
        
        if staging_articles == 0:
            logger.error(
                f"No staging data found for run_id {run_id}. "
                "Run run_newsapi_ai_job.py first."
            )
            raise SystemExit(1)
        
        logger.info(f"âœ“ Staging data ready: {staging_articles} articles for run_id {run_id}")
        
    finally:
        con.close()
    
    logger.info("All pre-flight checks passed âœ“")

def run_step(cmd: list[str], step_name: str) -> str:
    """
    Execute a pipeline step with logging and error handling.
    
    Args:
        cmd: Command to execute as list of strings
        step_name: Human-readable name for logging
        
    Returns:
        stdout from the command
        
    Raises:
        SystemExit: If command fails with non-zero exit code
    """
    logger.info(f"Starting step: {step_name}")
    start_time = time.time()
    
    try:
        result = subprocess.run(
            cmd,
            cwd=str(PROJECT_ROOT),
            capture_output=True,
            text=True,
            check=False
        )
        
        elapsed = time.time() - start_time
        
        if result.returncode != 0:
            logger.error(f"âŒ {step_name} FAILED (exit code {result.returncode})")
            logger.error(f"STDERR:\n{result.stderr}")
            logger.error(f"STDOUT:\n{result.stdout}")
            raise SystemExit(result.returncode)
        
        logger.info(f"âœ… {step_name} completed in {elapsed:.2f}s")
        
        # Log key metrics from stdout if present
        if "Extracted=" in result.stdout:
            logger.info(f"Output: {result.stdout.strip()}")
        
        return result.stdout
        
    except Exception as e:
        logger.exception(f"ğŸ’¥ {step_name} CRASHED: {e}")
        raise SystemExit(1)

def main():
    parser = argparse.ArgumentParser(
        description="Run end-to-end incidents extraction pipeline"
    )
    parser.add_argument(
        "--run-id",
        required=True,
        help="Unique identifier for this ingestion run"
    )
    args = parser.parse_args()
    
    logger.info(f"========== INCIDENTS PIPELINE START ==========")
    logger.info(f"Run ID: {args.run_id}")
    
    pipeline_start = time.time()
    
    try:
        # Pre-flight validation
        preflight_checks(args.run_id)
        
        # Step 1: Extract incidents from staging news
        run_step(
            [PYTHON, "scripts/run_incident_extract_baseline.py", "--run-id", args.run_id],
            "Incident Extraction"
        )
        
        # Step 2: Resolve geocoding
        run_step(
            [PYTHON, "scripts/run_geo_resolve_incidents.py", "--run-id", args.run_id],
            "GEO Resolution"
        )
        
        # Step 3: Build fact table
        run_step(
            [PYTHON, "scripts/build_fct_incidents.py", "--run-id", args.run_id],
            "Fact Table Build"
        )
        
        # Step 4: Run data quality checks
        run_step(
            [PYTHON, "scripts/check_incidents_run.py", "--run-id", args.run_id],
            "Data Quality Checks"
        )
        
        total_elapsed = time.time() - pipeline_start
        logger.info(f"========== PIPELINE SUCCESS ==========")
        logger.info(f"Total duration: {total_elapsed:.2f}s")
        logger.info(f"Run ID: {args.run_id}")
        
        print("EXIT_CODE=0")
        
    except SystemExit as e:
        total_elapsed = time.time() - pipeline_start
        logger.error(f"========== PIPELINE FAILED ==========")
        logger.error(f"Duration before failure: {total_elapsed:.2f}s")
        logger.error(f"Run ID: {args.run_id}")
        logger.error(f"Exit code: {e.code}")
        raise

if __name__ == "__main__":
    main()
```

### Mejora propuesta: inspect_dim_places_pe.py con schema real
```python
import duckdb
from pathlib import Path

def inspect_dim_places():
    """
    Inspect dim_places_pe table structure and contents.
    """
    db_path = Path(__file__).resolve().parents[1] / "data" / "osint_dw.duckdb"
    con = duckdb.connect(str(db_path))
    
    try:
        print("=" * 80)
        print("DIM_PLACES_PE - TABLE SCHEMA")
        print("=" * 80)
        schema_df = con.execute("DESCRIBE dim_places_pe").df()
        print(schema_df.to_string(index=False))
        
        print("\n" + "=" * 80)
        print("DIM_PLACES_PE - STATISTICS")
        print("=" * 80)
        stats_df = con.execute("""
            SELECT 
                COUNT(*) as total_places,
                COUNT(DISTINCT adm1_name) as departments,
                COUNT(DISTINCT adm2_name) as provinces,
                COUNT(DISTINCT adm3_name) as districts,
                COUNT(DISTINCT CASE WHEN lat IS NOT NULL THEN place_id END) as geocoded_places,
                MIN(lat) as min_lat,
                MAX(lat) as max_lat,
                MIN(lon) as min_lon,
                MAX(lon) as max_lon
            FROM dim_places_pe
        """).df()
        print(stats_df.to_string(index=False))
        
        print("\n" + "=" * 80)
        print("DIM_PLACES_PE - SAMPLE ROWS")
        print("=" * 80)
        sample_df = con.execute("""
            SELECT
                place_id, 
                adm1_name, 
                adm2_name, 
                adm3_name, 
                lat, 
                lon
            FROM dim_places_pe
            LIMIT 10
        """).df()
        print(sample_df.to_string(index=False))
        
    finally:
        con.close()

if __name__ == "__main__":
    inspect_dim_places()
```

---

## 12. COMANDOS ÃšTILES

```bash
# Activar virtual environment
.\.venv\Scripts\Activate.ps1

# Inspeccionar schemas reales de tablas DuckDB
python -c "import duckdb; con=duckdb.connect('data/osint_dw.duckdb'); print(con.execute('DESCRIBE dim_places_pe').df())"

# Ver todas las tablas en DuckDB
python -c "import duckdb; con=duckdb.connect('data/osint_dw.duckdb'); print(con.execute('SHOW TABLES').df())"

# Ejecutar pipeline completo de incidentes
.\.venv\Scripts\python.exe scripts\run_incidents_job.py --run-id 20251214165627

# Ejecutar solo un paso especÃ­fico del pipeline
.\.venv\Scripts\python.exe scripts\run_geo_resolve_incidents.py --run-id 20251214165627

# Validar integridad del gazetteer
python scripts\validate_gazetteer_checksum.py

# Cargar/recargar dimensiÃ³n geogrÃ¡fica
python scripts\reset_dim_places_pe.py
python scripts\load_gazetteer_pe.py

# Ver estructura completa del proyecto
Get-ChildItem -Recurse .\scripts\*.py | Select-Object Name, Length, LastWriteTime

# Inspeccionar tablas especÃ­ficas
python scripts\inspect_dim_places_pe.py
python scripts\inspect_stg_incidents_extracted.py

# Verificar logs de ejecuciÃ³n (si existen)
Get-Content logs\*.log -Tail 50

# Ejecutar job completo NewsAPI â†’ Incidents (scheduled)
.\scripts\scheduled_run_newsapi_ai_job.ps1
```

---

## 13. ENLACES Y RECURSOS

| Recurso | URL | Notas |
|---------|-----|-------|
| NewsAPI.ai Documentation | https://newsapi.ai/documentation | Event Registry API reference |
| DuckDB Documentation | https://duckdb.org/docs/ | SQL reference y funciones |
| Claude API Documentation | https://docs.anthropic.com/ | Haiku model para classification |
| Azure OpenAI Service | https://learn.microsoft.com/azure/ai-services/openai/ | Setup guide |
| Azure AI Search | https://learn.microsoft.com/azure/search/ | RAG implementation |
| Azure Maps | https://learn.microsoft.com/azure/azure-maps/ | Geocoding services |
| Loguru Documentation | https://loguru.readthedocs.io/ | Python logging library |
| Medallion Architecture | https://www.databricks.com/glossary/medallion-architecture | Bronze/Silver/Gold pattern |

---

## 14. NOTAS PARA EL PRÃ“XIMO CHAT

### âš ï¸ Trampas / Cosas que costÃ³ descubrir
- **Gazetteer download en ejecuciÃ³n diaria causa HTTP 403** â†’ Versionar localmente con SHA256
- **Location-based queries dan cobertura terrible** â†’ Source-based ingestion fue game changer (6â†’650 artÃ­culos)
- **Rule-based classification es 90% ruido** â†’ LLM-based con Claude Haiku filtrÃ³ 71% efectivamente
- **`pragma_table_info` count â‰  schema real** â†’ Usar `DESCRIBE table_name` siempre
- **Subprocess sin capture_output = logs ciegos** â†’ Siempre capture stdout/stderr para debugging
- **Scripts legacy coexisten con actuales** â†’ Verificar fecha modificaciÃ³n antes de ejecutar cualquier script

### ğŸ’¡ Tips importantes
- **DuckDB es embeddable** â†’ No requiere server, perfecto para local development
- **Medallion architecture vale la pena** â†’ Permite rollback, auditorÃ­a, y analytics complejos
- **run_id como UUID universal** â†’ Propagarlo por TODA la pipeline garantiza trazabilidad completa
- **Pre-flight checks previenen desastres** â†’ Validar dim_places_pe, run_id, staging data ANTES de procesar
- **Loguru >> print()** â†’ Timestamps automÃ¡ticos, rotation,